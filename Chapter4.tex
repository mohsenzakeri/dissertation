%Chapter 4

\renewcommand{\thechapter}{4}

\chapter[Improving the Posterior Estimates]{Estimating the posterior with an improved 
version of bootstrap sampling based on Equivalence Classes} 
\label{chapt:boots}

\section{Overview}
% talk about bayesian bootstraps
Rubin \cite{rubin1981bayesian} introduced the Bayesian Bootstrap procedure, which generalizes 
the bootstrap and introduces a procedure for placing a prior over the sampling weights used in 
bootstrap resampling. The classic bootstrap is the posterior mean of the bayesian bootstrap, and 
Rubin demonstrated they have quite similar estimation properties, generality, and similar 
limitations.

In particular, in the discussion of the paper introducing the Bayesian Bootstrap, when 
discussing model specification, Rubin muses:

\textit{``is it reasonable to use a model specification that effectively assumes all possible distinct 
values of X have been observed?"}

Specifically, both the non-parametric bootstrap and the Bayesian Bootstrap make this assumption — 
no unobserved value will ever be included in a bootstrap replicate. This renders unobserved 
values “impossible” under the model, and prevents understanding the effect they might have on 
the inference procedure or the estimator being computed.

% talk about how we wanna estimate the bayesian bootstraps
Here, we introduce the notion of the Augmented Bootstrap. This procedure follows the general 
framework of the bootstrap (or, optionally, the Bayesian Bootstrap), but augments the observed 
data with additional “pseduo-observations” that represent values that are possible given a 
conceptual model for data generation, but which were not observed in the sample.

\section{Methods}

We will explain this idea in the underlying context of the problem of quantifying the abundance 
of transcripts from RNA-sequencing (RNA-seq) data.  In this problem, we observe a collection of 
sequenced fragments, from which we can then estimate the abundance of the distinct transcripts 
using an expectation maximization procedure (our "estimator" in the case of the bootstrap).  
Consider that we observe a collection $\mathcal{F}$ of $N$ sequenced fragments. Each fragment aligns 
to a set of distinct locations which we denote as $a(f_i) = \{(j_1, k_1), (j_2, k_2), \dots\}$ 
signifying that fragment $f_i$ aligns to transcript $j_1$ at position $k_1$, to transcript $j_2$ 
at position $k_2$, and soforth.  For a given set $\mathbf{\eta}$ of transcript abundances, we can 
write the probability of observing the set $\mathcal{F}$ of fragments as:

$$
\mathcal{L} = \prod_{i=1}^{N} \sum_{(j,k) \in a(f_i)} P(t_j|\mathbf{\eta}) P(f_i|t_{j,k}=1)
$$

where $P(t_j|\mathbf{\eta})$ is the probability of selecting transcript $j$ for sequencing 
conditional on the transcript abundances and $P(f_i|t_{j,k}=1)$ is the probability of generating 
fragment $i$ from transcript $j$ at position $k$.

\textcolor{red}{is there a way to put $k$ on the left side of the conditional bar? it seems funny on the 
right side in the second term}

While there is no closed-form solution to determine the $\mathbf{\eta}$ that maximize $\mathcal{L}$, 
we can, at least locally, optimize this quantity using an EM algorithm.  We are then interested in 
assessing the accuracy of $\mathbf{\hat{\eta}}$, our maximum likelihood estimator for $\mathbf{\eta}$.

To assess this accuracy, we can use the non-parametric bootstrap. In this framework, we will 
resample (with replacement) from $\mathcal{F}$ to produce a series of bootstrap replicates 
$\{\mathcal{F_1}, \mathcal{F_2}, \dots \mathcal{F_b}\}$, and for each we can use the EM procedure 
to obtain a maximum likelihood estimate of the transcript abundances given this bootstrap replicate; 
we denote these esitimates as 
$\{\mathbf{\hat{\eta_1}}, \mathbf{\hat{\eta_2}}, \dots, \mathbf{\hat{\eta_b}}\}$. 
We can then assess e.g. the variance of the estimate for the abundance of transcript $j$, denoted as 
$\hat{\eta}^{(j)}$, by assessing the sample variance of 
$\{ \hat{\eta_1}^{(j)}, \hat{\eta_2}^{(j)}, \dots, \hat{\eta_b}^{(j)} \}$.

Because we are resampling \textit{sequenced fragments} with replacement, we will never observe in our 
resampling a pattern of alignments for a fragment different from what we saw in our original sample 
$\mathcal{F}$. This leads to some interesting, and perhaps undesirable behavior of the bootstrap. 

Consider a pair of alleles of a transcript that differ only at a single locus.  Further, imagine 
that these transcripts are sequence distinct from the rest of the transcriptome (i.e. they share no 
multimapping reads with other transcripts apart from their sibling allele).  Let there be $N_t$ 
reads that map to both of these transcripts, and $0$ reads that map uniquely to either (i.e. 0 
reads that overlap the variant locus).

From the perspective of our estimator, these transcripts are inferentially indistinguishable.  
Specifically, with no prior information on whether one of these alleles is \textit{a priori} more likely 
than the other, we have no information about how the $N_t$ fragments should be allocated among 
these transcripts.  Perhaps $t_p$ and $t_m$ each give rise to $\frac{N_t}{2}$ fragments (what the 
EM will likely tell us), or perhaps $t_p$ gives rise to 0 fragments and $t_m$ gives rise to all 
$N_t$ fragments.  In fact, any combintation between the two alleles that sums to $N_t$ is feasible 
and has equal likelihood.

The crux of the issue in uncovering this uncertainty using the non-parametric bootstrap (either the 
traditional or the Bayesian variant), is that no observation was made that distinguishes between 
these alleles.  Thus, no matter how we resample the original observations, we will never be able 
to recover the underlying uncertainty in the abundance of these transcripts.  Our estimator will 
demonstrate some variance over the bootstrap replicates, of course, but only related to what 
fraction of the original $N_t$ reads we sample within each replicate (with the expected value, 
of course, being $N_t$).

\subsection{The Augmented Bootstrap}

To address this issue, we propose the augmented bootstrap.  This procedure is applicable in 
sutations where the data over which inference is being performed have a finite (and ideally "small") 
support **Mike: I think of the parameter having support, not data**  We describe the procedure in 
full generality, and then explain the heuristics that we adopt to make the procedure computationally 
expedient in our use case.

The main idea behind the augmented bootstrap is that we will augment our observed data with some set 
of "pseudo-observations" --- data values that \textit{might} have been observed, but which were not observed 
in our sample.  This is analogous to placing a prior over the discrete set of possible observations 
that may be made. The prior may be informative or noninformative, and this can represented by means 
of the sampling weights assigned to each of these pseudo-observations.

Let us consider our chosen problem of transcript abundance from RNA sequencing data. To simplify the 
exposition, let us further assume we are dealing with single-end reads, and that we will ignore the 
possibility of sequencing error when generating our "pseudo-observations". Then, in this case we 
may consider producing a set of pseudo-observations by drawing, from every transcript, a fragment 
starting at every position.  Let this set of pseudo-observations be denoted as $\mathcal{P}$ and 
let us denote by $\mathcal{F_{A}}$ the set $\mathcal{F} \cup \mathcal{P}$. This is our collection 
of augmented observations — the set of samples over which we will perform our augmented 
bootstrapping procedure.  While we could consider every observation $f \in \mathcal{F_{A}}$ to be 
sampled with replacement with equal probability, this introduces an obvious dependency between 
$\left|\mathcal{F}\right|$ and $\left|\mathcal{P}\right|$ where the effect of the 
pseudo-observations will be relatively larger when the original sample is small and less important 
when the original saple is very large.  Thus, we will consider modifying the sampling weights 
between "true" observations (those in $\mathcal{F}$ ) and pseudo-observations 
(those in $\mathcal{P}$).  While we can consider giving each pseudo-observation a distinct sampling 
weight, let us consider here the simpler case where we define $w < 1$ to be the sampling weight 
applied to every pseudo-observation.  In fact, this gives us a direct way to interpret the weight 
of all pseudo-observations as a proportion of the weight of the true observations.  Consider that 
we want the pseudo-observations to account for $1\%$ of the samples in a given bootstrap sample — 
then we can set the weights in the following way.

Let $\left|\mathcal{F}\right| = N$ and $\left| \mathcal{P} \right| = N'$ so taht 
$\left| \mathcal{F_{A}} \right| = N + N'$.  We would like the weight of each pseudo-observation 
to be proportional to $w$ times the weight of each true datum, where $0 \le w \le 1$. 
% Then we can say $\gamma = \frac{N}{N+N'} + \frac{w N'}{N+N'}$.

Now, when we perform a bootstrap replicate, we wish to resample with replacement from 
$\mathcal{F_{A}} = \langle f_1, f_2, \dots, f_N, p_1, p_2, \dots, p_{N'} \rangle$ where we will use 
the sampling weights $\langle y_1, y_2, \dots, y_N, z_1, z_2, \dots, z_{N'}\rangle$ where 

$$
% y_i = \frac{\gamma^{-1}}{N+N'}
y_i = \frac{(1-w) (N+N')}{N}
$$

% and 

$$
% z_i = \frac{w \gamma^{-1}}{N+N'}
z_i = \frac{w (N+N')}{N'}
$$

% This gives every pseudo-observation a relative sampling weight equal to $w$ times the sampling weight 
% of a true observation. 
This gives the relative sampling weight of every pseudo-observation and every true observation
equal to $w$ and $1-w$ respectively. Thus, in expectation, $(100 \times w)\%$ of our sampled data 
in each bootstrap replicate will consist of pseudo-observations, while the rest will consist of 
true observations.

The effect of adding these pseudo-observations to augment our bootstrap sampling is that we can now 
observe outcomes in our estimates that previously would have not been possible due to plausible 
observations that were missing from our specific sample.

Returning to the running example of the alleles of a single transcript; in addition to the $N_t$ 
fragments compatible with both alleles, there will now be pseudo-observations compatible only with 
$t_m$ and pseudo-observations compatible only with $t_p$.  In a given bootstrap replicate the 
inclusion or exclusion of these pseudo-observations will result in a different relative estimate 
between the abundances of these two alleles than we will ever arrive at under a bootstrap replicate 
of the original data.  In a sense, this augmentation is enabling us to approach the bootstrap 
procedure from a more Bayesian perspective, where data are \textit{possible} even when they are not 
observed.  The cost for this, of course, is that we must make some decision about their prior probability.

\subsection{Heuristics for augmenting the bootstrap}

We have defined the augmented bootsrap procedure as augmenting the observed sample with 
pseudo-observations for \textit{all possible} observations. This can immediately pose some challenges. 
First, it requires that the set of possible observations is finite and sufficiently small to be 
enumerated. Second, many possible pseudo-observations, though technically possible given the 
imposed prior, may have little to no effect on the resulting inference of interest.

Again, consider our running example of transcript abundance estimation. Here, under observations 
of the original sample, many transcripts will both have an estimated abundance of 0 and, further, 
will simply have no observed sequenced fragments compatible with them.  Generating and possibly 
sampling pseudo-observations from these transcripts may lead to small fluctuations in the estimated 
abundance of these transcripts across bootstrap replicates, but it is unlikely to have any 
substantial effect on the "main" inference problem (i.e. the estimated maximum likelihood 
abundances of non-trivially expressed transcripts) — and since no "true" observations are 
compatible with these transcripts, we'd expect their posterior samples to be rather uninteresting.

This immediately suggests one potential heuristic for limiting the number of pseudo-observations 
with which we will augment our true samples. Let $\mathcal{T}$ be our complete set of transcripts 
and let $\mathcal{T}_\mathcal{F}$ be the set of transcripts having at least one fragment in 
$\mathcal{F}$ that aligns to them. Rather than generating pseudo-observations from all of 
$\mathcal{T}$, we may consider generating pseudo-observations only from 
$\mathcal{T}_\mathcal{F}$ — that is from transcripts that we predict to be expressed and from those 
that have compatible observations (but which we may not predict to be expressed in the maximum 
likelihood estimate). In general, this will produce far fewer pseudo-observations than if we 
generate them from all of $\mathcal{T}$. Furthermore, the sampling of \textit{these} 
pseudo-observations are much more likely to lead to alternative high-likelihood estimates across 
bootstrap-replicates, because they are most likely to change the balance of observations in 
highly-ambiguous components within the inference problem. Thus, we are, in effect, selecting a 
smaller set of pseudo-observations that are more likely to uncover the relevant uncertainty 
in our estimator.

\textcolor{red}{talk about just sampling from the single-transcript equivalence classes — that is, 
we can take this idea one step further by, instead of generating pseudo-observations only from 
$\mathcal{T}_\mathcal{F}$, only generating those \textit{specific} pseudo-observations from along 
the transcripts in $\mathcal{T}_\mathcal{F}$ that are unique to each transcript.}



\section{Results}

\subsection{Estimating the allelic expression}
We have created a simulated dataset of Drosophila with expression of the reads from two alleles of 
each gene. To create the allelic reads, we flip a random base in the original genes and then generate
reads with Polyester RNA-seq simulator\cite{frazee2015polyester} from both allelic genes. Based on the 
procedure for generating the allelic reads, most, if not all, the reads mapping to a transcript, map 
to both its paternal and maternal alleles.

First of all, we demonstrate how the regular \boots fails to capture the entire posterior distributions
for many \txps in this sample. We create inferential replicates by \boots available in \salmon and 
\kallisto. We also create \boot samples using the \aboots with different weights. We \salmon in two
different optimization modes of `EM' and `VBEM'. The `EM' mode is the regular expected maximization 
algorithm and the `VBEM' is a variational bayesian version of the EM with a non-zero priors for the 
model's parameters which induces further sparsity on the final solution.

\begin{table} \centering
    \begin{tabular}{lrrrr}
    \toprule
    {}  & Spearman-all & MARD-all  & MARD-subset & Spearman-subset \\
    \midrule
    \kallisto & \num{0.9393075} & \num{0} & \num{0} & \num{0} \\
    \kallisto \boot & \num{0.942153293871728} & \num{0} & \num{0} & \num{0} \\
    \salmon EM point-estimates & \num{0.9537012} & \num{0.} & \num{0.} & \num{0.} \\
    \salmon VBEM point-estimates & \num{0.7967372} & \num{0} & \num{0.} & \num{0.} \\
    \salmon EM \boot & \num{0.986101} & \num{0} & \num{0.}  & \num{0.} \\
    \salmon VBEM \boot  & \num{0.9010147} & \num{0} & \num{0.}  & \num{0.} \\
    \salmon VBEM \aboot (w=0.005) & \num{0.977737} & \num{0.} & \num{0.} & \num{0.} \\
    \salmon VBEM \aboot (w=0.01) & \num{0.9852121} & \num{0.} & \num{0} & \num{0.} \\
    \salmon VBEM \aboot (w=0.1) & \num{0.9924701} & \num{0.} & \num{0} & \num{0.} \\
    \bottomrule
    \end{tabular}
    \captionof{table}[Quantification of allelic transcriptome]{}
    \label{tab:allele}
\end{table}
